---
title: "Methods for the Classification of Data from Open-Ended Questions in Surveys"
subtitle: "Disputation<br>16 April 2024"
author:
  - name: Camille Landesvatter
    #email: camille.landesvatter@uni-mannheim.de
    affiliations:
      - name: University of Mannheim
format:
  revealjs: 
    theme: [white, custom.css]
    #theme: default 
    #theme: simple
    footer: "Landesvatter: Methods for the Classification of Data from Open-Ended Questions in Surveys"
    embed-resources: true
    slide-number: true
    preview-links: auto
    #logo: images/quarto.png
    #show-slide-number: print
editor: 
  markdown: 
    wrap: 72
---

## Research Objective

<font size = "6">

<blockquote>"\[...\] [introducing various methods]{style="color:blue"}
of classifying data from open-ended survey questions and [empirically
illustrating their application]{style="color:blue"}.\
A central research question addressed in this thesis therefore concerns
the analysis of [(short) text data generated by open-ended survey
questions]{style="color:blue"}." (Landesvatter 2023, p.2)</blockquote>

</font size>

------------------------------------------------------------------------

## Terminology: Open-Ended Questions in Surveys

<font size = "6">

- "survey questions that do not include a set of response options” (Züll, 2016, p. 1)

- "require respondents to formulate a response in their own words and to express it verbally or in writing” (Züll, 2016, p. 1)

- ≠ closed-ended questions with answer categories presented in a closed form (Inui et al., 2001, p. 1)

<!-- <blockquote>descriptive, who, what, when, where, and why questions -->
<!-- (Popping, 2015)</blockquote> -->

</font size>

------------------------------------------------------------------------

## Strategy

<font size = "5">

<blockquote>"\[...\] [introducing various methods]{style="color:blue"} of classifying data from open-ended survey questions and [empirically illustrating their application]{style="color:blue"}. A central research question addressed in this thesis therefore concerns the analysis of [(short) text data generated by open-ended survey questions]{style="color:blue"}."</blockquote>

</font size>

<font size = "6">

1. introducing readers to the survey methodology of using open-ended questions
	- including historical and modern developments, characteristics and challenges of open-ended questions, types of OEQs (e.g., probing)
	
2. introducing readers to computational methods available for analysis of open-ended answers
	- manual, semi-automated, fully automated

3. applying various of these available methods in three empirical studies

</font size>

------------------------------------------------------------------------

## Methods for Analyzing Data from Open-Ended Questions

<!-- The quantitative analysis of data from open-ended questions, which means formatting the unstructured natural language data into numerical formats, requires methods of classification. -->

<!-- ![Table 1. Overview of methods for classifying open-ended survey -->
<!-- responses](table-methods.png "Overview of methods for classifying open-ended survey responses"){fig-alt="Overview of methods for classifying open-ended survey responses."} -->

![Table 1. Overview of methods for classifying open-ended survey
responses](table-methods-colored.png "Overview of methods for classifying open-ended survey responses"){fig-alt="Overview of methods for classifying open-ended survey responses."}


------------------------------------------------------------------------

## Motivation

<font size = "6">

➡️ The increase in methods to collect open-ended answers (e.g. smartphone-administered surveys, voice technologies, novel methods) calls for testing and validating automated methods to analyze the resulting data

❓ Why did I chose the Survey Context?

❓ Why do I focus on computational methods?

</font size>

------------------------------------------------------------------------

## Motivation: Why Survey Context?

<font size = "6">

- data from OEQs represent a special and intriguing type of data for ML applications due to their short, concise and low in context textual data 
- this can require the use of suitable methods, e.g., word embeddings, structural topic models
	
<!-- Often (except for respondents that repeat the question wording in their answer) the context is only included in the survey question. Also, in open-ended survey answers, content is “related to a theme more specific to a certain field than politics, finance or society in newspapers” -->

</font size>

```{r wordcloud-survey-answers, out.width='100%', fig.align = "center", fig.show='hold'}
#| label: fig-1
#| fig-show: "asis"
#| fig-cap: "The previous question was: 'How often can you trust the federal government in Washington to do what is right?'. Your answer was: '[Always; Most of the time; About half of the time; Some of the time; Never; Don’t Know]'. In your own words, please explain why you selected this answer."

library(tidyverse)

setwd("/Users/camillelandesvatter/Library/CloudStorage/GoogleDrive-landesvatterc@gmail.com/Meine Ablage/2022_work-life-politics/main study/Paper Text-Audio/Submission 1 (POQ)")

data <- read.csv("./data/data_long.csv") %>% 
  filter(variable_probing=="political_trust_washington") %>% 
	select("ID_participant", "text_answer") %>% 
	filter(!is.na(text_answer)) %>% 
	mutate(doc_length=ifelse(!is.na(text_answer),sapply(strsplit(as.character(text_answer), "[[:space:]]+"), length),NA)) %>% 
	arrange(doc_length) %>% 
	filter(ID_participant == "5d91daf3704e790018f47229" | # no context
				 	ID_participant == "61119010fe205e4436dce3d6" | # no context
				 	ID_participant == "5d62b2186f363200168bbb85" | # issues of self-administered questionnaires
				 	ID_participant == "5ea83c900b2b5d24b9942af2" | # issues of self-administered questionnaires
				 	ID_participant == "601cbb3526aa517be4a924ce" | # context is provided but no details
				 	ID_participant == "63647038cdac73d9a21f4a3c" |
				 	ID_participant == "5f5a0f0a612008057e58d281" | #useful
				 	ID_participant == "5d42ec552de85600153b604a" | #useful
				 	ID_participant == "5ebf2b97ce2e0f1c05e9b00a" #useful, but other issues (too lengthy)
				 ) %>% 
	mutate(code=as.character(c(1,1,2,3,1,2,4,4,5)))


# Create a plot
library(ggplot2)
library(ggwordcloud)

set.seed(42)
ggplot(data, aes(label = text_answer)) +
  geom_text_wordcloud(aes(color = code, size=7)) +
	scale_size_area(max_size = 6) +
  theme_minimal() +
  #labs(title = "") +
  theme(legend.position = "none") +
	coord_equal(ratio = 0.5) +
	theme_linedraw()

```



------------------------------------------------------------------------

## Motivation: Why Computational Methods?

<font size = "6">

- fully manual methods require high ressources (time and effort)

- but more importantly, human codings can
	- be biased (Mosca et al., 2022),
	- lack objectivity (Inui et al., 2001),
	- introduce errors when coders misinterpret answers or annotation codes (Giorgetti & Sebastiani, 2003),
	- face transparency issues related to unitization and intercoder reliability (Campbell et al., 2013).
	
- automated methods offer objectivity and systematicness (Zhang et al., 2022)

- still, issues persist (e.g., transparency) which makes it crucial to test and evaluate methods for the social sciences


</font size>

------------------------------------------------------------------------

## <font size = "14"> Empirical Contributions </font size> {.center}

------------------------------------------------------------------------

## Empirical Contributions: Overview

<font size = "6">


| 1             | 2             | 3            |
|---------------|---------------|--------------|
| How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning | Open-ended survey questions: A comparison of information content in text and audio response format | Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? |

-   data collection approach: three self-administered web surveys with
    open-ended questions
- 	data from three U.S. non-probability samples
-   methodology for text classification: supervised ML, unsupervised ML,
    fine-tuning of pre-trained language model BERT, zero-shot learning
    
</font size>

------------------------------------------------------------------------

## How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning {background-image="paper1/theory.png" background-size="80%" background-opacity="0.4"}


**Co-authored by**: Dr. Paul C. Bauer

**Published In**: Landesvatter, C., & Bauer, P. C. (2024). How Valid Are Trust Survey Measures? New Insights From Open-Ended Probing Data and Supervised Machine Learning. Sociological Methods & Research, 0(0). https://doi.org/10.1177/00491241241234871

<!-- <img src="paper1/SMR_screenshot.png" alt="SMR" width="950" height="300"> -->

<!-- style="opacity: 0.8; -->




------------------------------------------------------------------------

## The validity of trust survey measures: Background

<font size = "6">

- Background:
	- ongoing debates about which type of trust survey researchers are measuring with traditional survey items (i.e., equivalence debate cf. Bauer & Freitag 2018)

- Research Question:
	- How valid are traditional trust survey measures?

- Experimental Design:
	- block randomized question order where closed-ended questions are followed by open-ended follow-up probing questions

</font size>

------------------------------------------------------------------------

## The validity of trust survey measures: Methodology

<font size = "6">

- Operationalization via two classifications: share of known vs. unknown others in associations (I), sentiment (pos-neu-neg) of assocations (II)
- Supervised classification approach:
	- 1. manual labeling of randomly sampled documents (n=[1,000,1,500])
	- 2. fine-tuning the weights of two BERT^[bidirectional encoder representations from transformers] models (base model uncased version), using the manually coded data as training data, to classify the remaining=[6,500/6,000]
	- accuracy^[acc = correct predictions / all predictions]: 87% (I) and 95% (II)
	
</font size>
	
<!-- what are acceptable and what are good accuracy scores? -->

------------------------------------------------------------------------

## The validity of trust survey measures: Results

<figure>
  <img src="paper1/results-1.png" alt="Paper 1 Results">
  <figcaption>Figure 1: Illustration of exemplary data.</figcaption>
</figure>

<!-- associations are meaningful and influence trust scores, especially particularized trust (trust in known others) has an impact despite we want to measure generalized trust! -->

<figure>
  <img src="paper1/results-2.png" width="710" height="280">
  <figcaption>Figure 2: Associations and trust scores across different measures.</figcaption>
</figure>




------------------------------------------------------------------------

## Open-ended survey questions: A comparison of information content in text and audio response formats {background-image="paper2/typing-vs-speaking.png" background-size="60%" background-opacity="0.4"}


**Co-authored by**: Dr. Paul C. Bauer

**Submitted to**: Public Opinion Quarterly in February 2024


<!-- <img src="paper2/typing-vs-speaking.png"> -->

<!-- style="opacity: 0.8; -->

------------------------------------------------------------------------

## Information content Text vs. Audio Responses: Background

<font size = "6">

- Background:
	- recent increase of voice-based response options in surveys due to mobile devices equipped with voice input technologies, smartphone surveys and speech-to-text technologies

- Research Question:
	- Are there differences in information content between responses given in voice and text formats?

- Experimental Design:
	- block randomized question order with open-ended and probing questions
	- random assignment into either the text or voice condition

</font size>

------------------------------------------------------------------------

## Information content Text vs. Audio Responses: Methodology

<font size = "6">

- Operationalization via application of measures from information theory and machine learning to classify open-ended survey answers
	- number of topics, response entropy
	- plus, response length
	
</font size>
	



------------------------------------------------------------------------

## Information content Text vs. Audio Responses: Results


<figure>
  <img src="paper2/overall-plot-disputation.png">
  <figcaption>Figure 3: Information Content Measures across questions.</figcaption>
</figure>



------------------------------------------------------------------------

## Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? {background-image="paper3/dreamstudio-emotion-analysis.png" background-size="40%" background-opacity="0.4"}

**Co-authored by**: Dr. Paul C. Bauer

**Submitted to**: American Political Science Review in March 2024

------------------------------------------------------------------------

## Affective Components in Political Trust: Background


<font size = "6">

- Background:
	- conventional notion stating tha trust originates from informed, rational, and consequential judgments is challenged by the idea of an "affective-based" form of (political) trust

- Research Question:
	- Are individual trust judgments in surveys driven by affective rationales?

- Questionnaire Design:
	- closed-ended political trust question followed by open-ended probing question


</font size>


------------------------------------------------------------------------

## Affective Components in Political Trust: Methodology

<font size = "6">

- Operationalization via sentiment and emotion analysis

- Transcript-based
	- pysentimiento for sentiment recognition (Pérez et al. 2023)
	- zero-shot prompting with GPT-3.5
- Speech-based
	- SpeechBrain for Speech Emotion Recognition (Ravanelli et al. 2021)
	
	
</font size>


------------------------------------------------------------------------


## Affective Components in Political Trust: Results


<!-- <figure> -->
<!--   <img src="paper3/results-sentiment.png" alt="Paper 3 Results" width="1000" height="510"> -->
<!--   <figcaption>Figure 5: Illustration of exemplary data.</figcaption> -->
<!-- </figure> -->

<!-- lets just talk about the most innovative part of this research: whether audio cues can deliever interesting insights -->

<figure>
  <img src="paper3/results-emotion.png" width="650" height="550" align="right">
  <figcaption>Figure 6: Results from Speech Emotion Recognition.</figcaption>
</figure>


------------------------------------------------------------------------

## Summary & Discussion

<font size = "6">

- web surveys can be used to collect narrative answers that provide valuable insights into survey responses

- various modern developments (smartphone surveys, speech-to-text algorithms) can be leveraged to collect such data in innovative ways (e.g., spoken answers)
	- always consider challenges and objectives (i.e., in term of sample sizes and sample compositions)

- computational measures can be applied to classify open-ended answers from surveys in order to inform ongoing debates in different fields, e.g.:
	- equivalence debate in trust research (Study 1), cognitive-versus-affective debate in political trust research (Study 3)
	- survey questionnaire design (Study 2) or item and data quality in general (e.g., associations, sentiment) (Study 1-3)
	
</font size>

------------------------------------------------------------------------

## Some observations and conclusions

<font size = "6">

<blockquote style="color:blue">Facilitated accessibility and implementation of semi-automated methods.</blockquote>

- supervised models have been a standard in automated methods, but recent developments of large and general-aim pre-trained  models (e.g., BERT) allow less resource-intensive fine-tuning

- For example, using only ~13% (1,000 documents from 7,500 in Study 1) documents for fine-tuning resulted in sufficient accuracy (i.e., 87%)
	- increasing the number of manually labeled documents can help in terms of accuracy (i.e., 92% in Study 1)
		- higher number of manual examples also improves the transparency of results: accuracy vs. transparency trade-off
			- start with simple methods and evaluate (e.g., Study 1, first Random Forest, only later BERT)
	

</font size>

------------------------------------------------------------------------

## Some observations and conclusions

<font size = "6">

<blockquote style="color:blue">Increase in possibilities of fully automated methods (e.g., prompt engineering.</blockquote>

- fully automated methods, such as zero-shot prompting can keep up with fine-tuned versions of pre-trained models (e.g., pysentimiento, Study 2)
	- deciding on a suitable number of manual examples and for a method in general (e.g., fully automated (unsupervised) versus semi-automated (supervised, and finetuning) depends on resources such as expected difficulty, desired accuracy, available time and cost resources
	

</font size>

<!-- - be cautious and aware when applying models that are not designed for survey data to survey data!  -->
<!-- - future research has a lot of evaluating to do -->
<!-- - plus, NLP no longer means text-only, audio data can be easily transcribed to text today and can deliever excting insights -->

<!-- Fragen von Florian beachten -->

<!-- https://docs.google.com/document/d/1tu6T91fbFi_wuRgepA6Okus_JVgIL3Rw0TZrhrCFpBs/edit -->

------------------------------------------------------------------------

## Thank you for your Attention!

<!-- note: host slides on github -->

<!-- picture: box with nlp, css, survey research, etc in lower right corner, https://sodas.ku.dk/projects/nlp-for-social-data-science/-->

------------------------------------------------------------------------

## References

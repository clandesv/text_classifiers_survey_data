---
title: "Methods for the Classification of Data from Open-Ended Questions in Surveys"
subtitle: "Disputation<br>16 April 2024"
author:
  - name: Camille Landesvatter
    #email: camille.landesvatter@uni-mannheim.de
    affiliations:
      - name: University of Mannheim
format:
  revealjs: 
    theme: [white, custom.css]
    #theme: default 
    #theme: simple
    footer: "Landesvatter: Methods for the Classification of Data from Open-Ended Questions in Surveys"
    embed-resources: true
    slide-number: true
    preview-links: auto
    #logo: images/quarto.png
    #show-slide-number: print
editor: 
  markdown: 
    wrap: 72
---

## Research Questions and Motivation

<font size = "6">

<blockquote> [Which methods can we use to classify data from open-ended survey questions?]{style="color:black"} </blockquote>

<blockquote> [Can we leverage these methods to make empirical contributions to substantial questions?]{style="color:black"} </blockquote>

<!-- <blockquote>"\[...\] [introducing various methods]{style="color:blue"} -->
<!-- of classifying data from open-ended survey questions and [empirically -->
<!-- illustrating their application]{style="color:blue"}.\ -->
<!-- A central research question addressed in this thesis therefore concerns -->
<!-- the analysis of [(short) text data generated by open-ended survey -->
<!-- questions]{style="color:blue"}." (Landesvatter 2023, p.2)</blockquote> -->

:::{.fragment}
Motivation:

➡️ The increase in methods to collect natural language (e.g., smartphone surveys and voice technologies) calls for testing and validating automated methods to analyze the resulting data.
:::

<!-- since nowadays we have this access to open-ended data, what is so special about them? -->

:::{.fragment}
➡️ Open-ended survey answers pose a unique challenge for ML applications due to their shortness and lack of context. An effective analysis might require the use of suitable methods, e.g., word embeddings, structural topic models.
:::

<!-- Often (except for respondents that repeat the question wording in their answer) the context is only included in the survey question. Also, in open-ended survey answers, content is “related to a theme more specific to a certain field than politics, finance or society in newspapers” -->

</font size>


------------------------------------------------------------------------

## Characteristics of Open-Ended Survey Answers

<!-- - "survey questions that do not include a set of response options” (Züll, 2016, p. 1) -->

<!-- - "require respondents to formulate a response in their own words and to express it verbally or in writing” (Züll, 2016, p. 1) -->

<!-- - ≠ closed-ended questions with answer categories presented in a closed form (Inui et al., 2001, p. 1) -->

```{r wordcloud-survey-answers, out.width='100%', fig.align = "center", fig.show='hold'}
#| label: fig-1
#| fig-show: "asis"
#| fig-cap: "The previous question was: 'How often can you trust the federal government in Washington to do what is right?'. Your answer was: '[Always; Most of the time; About half of the time; Some of the time; Never; Don’t Know]'. In your own words, please explain why you selected this answer."

library(tidyverse)

setwd("/Users/camillelandesvatter/Library/CloudStorage/GoogleDrive-landesvatterc@gmail.com/Meine Ablage/2022_work-life-politics/main study/Paper Text-Audio/Submission 1 (POQ)")

data <- read.csv("./data/data_long.csv") %>% 
  filter(variable_probing=="political_trust_washington") %>% 
	select("ID_participant", "text_answer") %>% 
	filter(!is.na(text_answer)) %>% 
	mutate(doc_length=ifelse(!is.na(text_answer),sapply(strsplit(as.character(text_answer), "[[:space:]]+"), length),NA)) %>% 
	arrange(doc_length) %>% 
	filter(ID_participant == "5d91daf3704e790018f47229" | # no context
				 	ID_participant == "61119010fe205e4436dce3d6" | # no context
				 	ID_participant == "5d62b2186f363200168bbb85" | # issues of self-administered questionnaires
				 	ID_participant == "5ea83c900b2b5d24b9942af2" | # issues of self-administered questionnaires
				 	ID_participant == "601cbb3526aa517be4a924ce" | # context is provided but no details
				 	ID_participant == "63647038cdac73d9a21f4a3c" |
				 	ID_participant == "5f5a0f0a612008057e58d281" | #useful
				 	ID_participant == "5d42ec552de85600153b604a" | #useful
				 	ID_participant == "5ebf2b97ce2e0f1c05e9b00a" #useful, but other issues (too lengthy)
				 ) %>% 
	mutate(code=as.character(c(1,1,2,3,1,2,4,4,5)))


# Create a plot
library(ggplot2)
library(ggwordcloud)

set.seed(42)
ggplot(data, aes(label = text_answer)) +
  geom_text_wordcloud(aes(color = code, size=7)) +
	scale_size_area(max_size = 6) +
  theme_minimal() +
  #labs(title = "") +
  theme(legend.position = "none") +
	coord_equal(ratio = 0.5) +
	theme_linedraw()

```


------------------------------------------------------------------------

## Structure and Approach of the dissertation

<font size = "6">

1. Introducing readers to the survey methodology of using open-ended questions (OEQs)
	- including historical and modern developments, characteristics and challenges of open-ended questions, types of OEQs (e.g., probing)

:::{.fragment}	
2. Introducing readers to computational methods available for analysis of open-ended answers
	- manual, semi-automated, fully automated
:::

:::{.fragment}
3. Applying the available methods in three empirical studies
:::

<!-- focus for todays talk is 2.) and 3.) -->

</font size>

------------------------------------------------------------------------

## Methods for Analyzing Data from Open-Ended Questions

<!-- The quantitative analysis of data from open-ended questions, which means formatting the unstructured natural language data into numerical formats, requires methods of classification. -->

<!-- ![Table 1. Overview of methods for classifying open-ended survey -->
<!-- responses](table-methods.png "Overview of methods for classifying open-ended survey responses"){fig-alt="Overview of methods for classifying open-ended survey responses."} -->

![Table 1. Overview of methods for classifying open-ended survey
responses](table-methods-colored.png "Overview of methods for classifying open-ended survey responses"){fig-alt="Overview of methods for classifying open-ended survey responses."}

<!-- fully manual methods require the highest resources (time and effort) -->

<!-- The only exception in Table 1, column 3 that does not involve manually annotated examples is zero-shot prompting. In zero-shot prompting, the human input does not consist of  annotated examples but merely a human-generated prompt. Zero-shot prompting can thus be considered a semi-automated approach, however with a minimum of human involvement. -->

------------------------------------------------------------------------

## <font size = "14"> Studies </font size> {.center}

------------------------------------------------------------------------

## Overview

<font size = "6">


| Study 1             | Study 2             | Study 3            |
|---------------|---------------|--------------|
| How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning | Open-ended survey questions: A comparison of information content in text and audio response format | Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? |

:::{.fragment}
- Data:
	- three self-administered web surveys with open-ended questions + U.S. non-probability samples
- Methodology for text classification:
	- supervised ML, unsupervised ML, fine-tuning of pre-trained language model BERT, zero-shot learning
:::
    
</font size>

------------------------------------------------------------------------

## How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning {background-image="paper1/theory.png" background-size="80%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (2024). How Valid Are Trust Survey Measures? New Insights From Open-Ended Probing Data and Supervised Machine Learning. Sociological Methods & Research, 0(0). https://doi.org/10.1177/00491241241234871 </blockquote>

<!-- <img src="paper1/SMR_screenshot.png" alt="SMR" width="950" height="300"> -->

<!-- style="opacity: 0.8; -->




------------------------------------------------------------------------

## Study 1: Background

<font size = "6">

- Background:
	- ongoing debates about which type of trust survey researchers are measuring with traditional survey items (i.e., equivalence debate cf. Bauer & Freitag 2018)

- Research Question:
	- How valid are traditional trust survey measures?

- Experimental Design:
	- block randomized question order where seven closed-ended questions are followed by open-ended follow-up probing questions
	
<!-- explain probe -->

</font size>

------------------------------------------------------------------------

## Study 1: Methodology

<font size = "6">

- Operationalization via two classifications: share of known vs. unknown others in associations (I), sentiment (pos-neu-neg) of assocations (II)
- Supervised classification approach:
	- 1. manual labeling of randomly sampled documents (n=[1,000,1,500])
	- 2. fine-tuning the weights of two BERT^[bidirectional encoder representations from transformers] models (base model uncased version), using the manually coded data as training data, to classify the remaining=[6,500/6,000]
	- accuracy^[accuracy = correct predictions / all predictions]: 87% (I) and 95% (II)
	
</font size>
	
<!-- what are acceptable and what are good accuracy scores? -->

------------------------------------------------------------------------

## Study 1: Results

<div style="text-align: center;">
<figure>
  <img src="paper1/table_exemplary_data.png">
  <figcaption>Figure 1: Illustration of exemplary data. <em>Note:</em> n=7,497.</figcaption>
</figure>

<!-- associations are meaningful and influence trust scores, especially particularized trust (trust in known others) has an impact despite we want to measure generalized trust! -->

:::{.fragment}
<figure>
  <img src="paper1/regression-plot-disputation.png">
  <figcaption>Figure 2: Trust Scores by Associations for the Most People Question.<br> <em>Note:</em> CIs are 90% and 95%, n=1,499.</figcaption>
</figure>
:::
</div>

<!-- sum(!is.na(data$probing_answer[data$variable=="Most people"])) -->


------------------------------------------------------------------------

## Open-ended survey questions: A comparison of information content in text and audio response formats {background-image="paper2/typing-vs-speaking.png" background-size="60%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (February 2024). Open-ended survey questions: A comparison of information content in text and audio response formats. [<u>Working Paper</u>]{style="color:black"} submitted to Public Opinion Quarterly. </blockquote>

<!-- <img src="paper2/typing-vs-speaking.png"> -->

<!-- style="opacity: 0.8; -->

------------------------------------------------------------------------

## Study 2: Background

<font size = "6">

- Background:
	- recent increase of voice-based response options in surveys due to mobile devices equipped with voice input technologies, smartphone surveys and speech-to-text technologies

- Research Question:
	- Are there differences in information content between responses given in voice and text formats?

- Experimental Design:
	- block randomized question order with open-ended and probing questions
	- random assignment into either the text or voice condition

</font size>

------------------------------------------------------------------------

## Study 2: Methodology

<font size = "6">

- Operationalization via application of measures from information theory and machine learning to classify open-ended survey answers
	- number of topics, response entropy
	- response length
	
</font size>
	



------------------------------------------------------------------------

## Study 2: Results

<div style="text-align: center;">
<figure>
  <img src="paper2/information-content-summary.png">
  <figcaption>Figure 3: Information Content Measures across Questions.<br>
  <em>Note.</em> CIs are 95%, n_vote-choice: 830 (audio: 225, text: 605), n_future-children: 1,337 (audio: 389, text: 748)</figcaption>
</figure>
</div>

<!-- data %>% -->
<!--     group_by(variable_probing, condition) %>% -->
<!--     summarise(non_missing_count = sum(!is.na(text_answer), na.rm = TRUE)) -->

------------------------------------------------------------------------

## Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? {background-image="paper3/dreamstudio-emotion-analysis.png" background-size="40%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (March 2024). Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys?. [<u>Working Paper</u>]{style="color:black"} submitted to American Political Science Review. </blockquote>

------------------------------------------------------------------------

## Study 3: Background


<font size = "6">

- Background:
	- conventional notion stating tha trust originates from informed, rational, and consequential judgments is challenged by the idea of an "affective-based" form of (political) trust

- Research Question:
	- Are individual trust judgments in surveys driven by affective rationales?

- Questionnaire Design:
	- closed-ended political trust question followed by open-ended probing question


</font size>


------------------------------------------------------------------------

## Study 3: Methodology

<font size = "6">

- Operationalization via sentiment and emotion analysis

- Transcript-based
	- pysentimiento for sentiment recognition (Pérez et al. 2023)
	- zero-shot prompting with GPT-3.5
- Speech-based
	- SpeechBrain for Speech Emotion Recognition (Ravanelli et al. 2021)
	
	
</font size>


------------------------------------------------------------------------


## Study 3: Results


<!-- <figure> -->
<!--   <img src="paper3/results-sentiment.png" alt="Paper 3 Results" width="1000" height="510"> -->
<!--   <figcaption>Figure 5: Illustration of exemplary data.</figcaption> -->
<!-- </figure> -->

<!-- lets just talk about the most innovative part of this research: whether audio cues can deliever interesting insights -->

<div style="text-align: center;">
<figure>
  <img src="paper3/results-emotion.png" width="650" height="550">
  <figcaption>Figure 4: Emotion Recognition for Speech Data with SpeechBrain. <em>Note.</em> CIs are 95%, n_neutral=408, n_anger=44, n_sadness=18, n_happiness=21.</figcaption>
</figure>
</div>

------------------------------------------------------------------------

## Summary 

<font size = "6">

- Web surveys allow to collect narrative answers that provide valuable insights into survey responses

:::{.fragment}
- Various modern developments (smartphone surveys, speech-to-text algorithms) can be leveraged to collect such data in innovative ways (e.g., spoken answers) (depends on goals! e.g., population)
:::

:::{.fragment}
- Natural language can be analyzed with computational measures which can inform debates in different fields, e.g.:
	- [Study 1]{style="color:blue"}: equivalence debate in trust research
	- [Study 3]{style="color:blue"}: cognitive-versus-affective debate in political trust research
	- [Study 2]{style="color:blue"}: survey questionnaire design or item and data quality in general (e.g., associations, sentiment, emotions) ([Study 1-3]{style="color:blue"})
:::

</font size>

------------------------------------------------------------------------

## Conclusion: Machine Learning and Open-ended Answers

<font size = "6">

<blockquote style="color:blue">Facilitated accessibility and implementation of semi-automated methods.</blockquote>

- large and general aim pre-trained models (e.g., BERT, GPT) allow less resource-intensive fine-tuning (compared to traditional supervised models) 

	- [Study 1]{style="color:orange"}: Manually labeling ~13% (n=1,000) documents for fine-tuning resulted in sufficient accuracy (87%)
	- Increasing the number of manually labeled documents can help in terms of accuracy (i.e., 92%) (and transparency)

- But: these models come along a lack of transparency
	- always start with simple methods and evaluate
		- [Study 1]{style="color:orange"}: Random Forest -> BERT
	- accuracy vs. transparency trade-off
	

</font size>

------------------------------------------------------------------------

## Conclusion

### Machine Learning and Open-Ended Answers


<!-- mention and cross out, topic models, dictionary approaches -->

<font size = "6">

<blockquote style="color:blue">Increasing number of possibilities of fully automated methods, for example prompt engineering.</blockquote>

-  [Study 2]{style="color:orange"}: fully automated methods, such as zero-shot prompting can keep up with fine-tuned versions of pre-trained models (e.g., GPT-prompting vs. pysentimiento)
	- deciding on a suitable number of manual examples depends on resources such as expected task difficulty, desired accuracy, available time and cost resources
	- ad. task difficulty, [Study 1]{style="color:orange"}: topic models (unsupervised) were too general to detect the fine nuances in content, we had to generate our own examples
	


</font size>

<!-- Fragen von Florian beachten -->

<!-- https://docs.google.com/document/d/1tu6T91fbFi_wuRgepA6Okus_JVgIL3Rw0TZrhrCFpBs/edit -->

- final decision depends on: difficulty of the given task, 
and for a method in general (e.g., fully automated (unsupervised) versus semi-automated (supervised, and finetuning) 
- plus, always rely on manua inspections of the classifications (in a systematic way!) (Study 2 (entropy), study 3 (sentiment with simple dict approach))
------------------------------------------------------------------------

## Thank you for your Attention!


<!-- picture: box with nlp, css, survey research, etc in lower right corner, https://sodas.ku.dk/projects/nlp-for-social-data-science/-->

------------------------------------------------------------------------

## References


<!-- Jan etc zitieren, oder auch: wer sagt emotions mit voice, oder wer sagt affect based pol trust -> die wichtigste Referenz zitieren -->

<!-- für jede substantielle frage die methodologische contribution (aus intro) erwähnen -->

<!-- genese zwischen den papieren beschreiben -->
---
title: "Methods for the Classification of Data from Open-Ended Questions in Surveys"
subtitle: "Disputation<br>16 April 2024"
author:
  - name: Camille Landesvatter
    affiliations:
      - name: University of Mannheim
format:
  revealjs: 
    theme: [white, custom.css]
    footer: "Landesvatter: Methods for the Classification of Data from Open-Ended Questions in Surveys"
    embed-resources: true
    slide-number: true
    preview-links: auto
    #logo: images/quarto.png
editor: 
  markdown: 
    wrap: 72
---

## Research Questions and Motivation

<font size = "6">

<blockquote> [Which methods can we use to classify data from open-ended survey questions?]{style="color:black"} </blockquote>

<blockquote> [Can we leverage these methods to make empirical contributions to substantial questions?]{style="color:black"} </blockquote>

:::{.fragment}
Motivation:

➡️ The increase in methods to collect natural language (e.g., smartphone surveys with voice technologies) calls for testing and validating automated methods to analyze the resulting data.
:::

:::{.fragment}
➡️ Open-ended survey answers pose a unique challenge for ML applications due to their shortness and lack of context. An effective analysis might require the use of suitable methods, e.g., word embeddings, structural topic models.
:::

</font size>

------------------------------------------------------------------------

## Methods for Analyzing Data from Open-Ended Questions

<!-- The quantitative analysis of data from open-ended questions, which means formatting the unstructured natural language data into numerical formats, requires methods of classification. -->

<!-- fully manual methods require the highest resources (time and effort) -->

<!-- The only exception in Table 1, column 3 that does not involve manually annotated examples is zero-shot prompting. In zero-shot prompting, the human input does not consist of  annotated examples but merely a human-generated prompt. Zero-shot prompting can thus be considered a semi-automated approach, however with a minimum of human involvement. -->

![Table 1. Overview of methods for classifying open-ended survey
responses. Source: Own depicition.](table-methods-colored.png "Overview of methods for classifying open-ended survey responses."){fig-alt="Overview of methods for classifying open-ended survey responses."}

------------------------------------------------------------------------

## Overview of Studies

<font size = "6">


| Study 1             | Study 2             | Study 3            |
|---------------|---------------|--------------|
| How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning | Open-ended survey questions: A comparison of information content in text and audio response formats | Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? |

</font size>

------------------------------------------------------------------------

## How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning {background-image="paper1/theory.png" background-size="80%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (2024). How Valid Are Trust Survey Measures? New Insights From Open-Ended Probing Data and Supervised Machine Learning. Sociological Methods & Research, 0(0). https://doi.org/10.1177/00491241241234871 </blockquote>

<!-- The first study is titled "..." and here I investigated whether we can rely on supervised machine learning to learn something about the measurement validity of survey questions -->




------------------------------------------------------------------------

## Study 1: Characteristics

<font size = "6">

- **Background**: ongoing debates about which type of trust survey researchers are measuring with traditional survey items (i.e., equivalence debate cf. Bauer & Freitag 2018^[Bauer, P. C., and M. Freitag. 2018. “Measuring Trust.” Pp. 1–27 in The Oxford Handbook of Social and Political Trust, edited by E. M. Uslaner. Oxford University Press.])

- **Research Question**: How valid are traditional trust survey measures?

- **Experimental Design**: block randomized question order where closed-ended questions are followed by open-ended follow-up probing questions

- **Data**: U.S. non-probability sample; n=1,500

</font size>

------------------------------------------------------------------------

## Study 1: Methodology

<font size = "6">

- Operationalization via two classifications: share of known vs. unknown others in associations (I), sentiment (pos-neu-neg) of assocations (II)
- Supervised classification approach:
	- 1. manual labeling of randomly sampled documents (n=[1,000/1,500])
	- 2. fine-tuning the weights of two BERT^[bidirectional encoder representations from transformers] models (base model uncased version), using the manually coded data as training data, to classify the remaining n=[6,500/6,000]
	- accuracy^[accuracy = correct predictions / all predictions]: 87% (I) and 95% (II)
	
</font size>


------------------------------------------------------------------------

## Study 1: Results

<div style="text-align: center;">
<figure>
  <img src="paper1/table_exemplary_data.png">
  <figcaption>Table 2: Illustration of exemplary data. <em>Note:</em> n=7,497.</figcaption>
</figure>

:::{.fragment}
<figure>
  <img src="paper1/regression-plot-disputation.png">
  <figcaption>Figure 1: Trust Scores by Associations for the Most People Question.<br> <em>Note:</em> CIs are 90% and 95%, n=1,499.</figcaption>
</figure>
:::
</div>

<!-- sum(!is.na(data$probing_answer[data$variable=="Most people"])) -->


------------------------------------------------------------------------

## Open-ended survey questions: A comparison of information content in text and audio response formats {background-image="paper2/typing-vs-speaking.png" background-size="60%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (February 2024). Open-ended survey questions: A comparison of information content in text and audio response formats. [<u>Working Paper</u>]{style="color:black"} submitted to Public Opinion Quarterly. </blockquote>

<!-- The second study is titled "..." and here I investigated whether we can use computational measures and ML to inform survey questionnaire design research -->

------------------------------------------------------------------------

## Study 2: Characteristics

<font size = "6">

- **Background**: requests for spoken answers are assumed to trigger an open narration with more intuitive and spontaneous answers (e.g., Gavras et al. 2022^[Gavras, K. et al. 2022. “Innovating the collection of open-ended answers: The linguistic and content characteristics of written and oral answers to political attitude questions.” Journal of the Royal Statistical Society. Series A,  185(3):872-890.])

- **Research Question**: Are there differences in information content between responses given in voice and text formats?

- **Experimental Design**: block randomized question order with open-ended (probing) questions; random assignment into either the text or voice condition

- **Data**: U.S. non-probability sample; n=1,461

</font size>

------------------------------------------------------------------------

## Study 2: Methodology

<font size = "6">

- Operationalization via application of measures from information theory and machine learning to classify open-ended survey answers
	- response length, number of topics, response entropy
	
</font size>
	



------------------------------------------------------------------------

## Study 2: Results

<div style="text-align: center;">
<figure>
  <img src="paper2/information-content-summary.png">
  <figcaption>Figure 2: Information Content Measures across Questions.<br>
  <em>Note.</em> CIs are 95%, n_vote-choice: 830 (audio: 225, text: 605), n_future-children: 1,337 (audio: 389, text: 748)</figcaption>
</figure>
</div>

<!-- data %>% -->
<!--     group_by(variable_probing, condition) %>% -->
<!--     summarise(non_missing_count = sum(!is.na(text_answer), na.rm = TRUE)) -->

------------------------------------------------------------------------

## Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys? {background-image="paper3/dreamstudio-emotion-analysis.png" background-size="40%" background-opacity="0.4"}

<blockquote> Landesvatter, C., & Bauer, P. C. (March 2024). Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys?. [<u>Working Paper</u>]{style="color:black"} submitted to American Political Science Review. </blockquote>

<!-- The third study is titled "..." and here I investigated whether I can use sentiment and emotion recognition and importantly speech emotion recognition to learn about political trust -->

------------------------------------------------------------------------

## Study 3: Characteristics

<font size = "6">

- **Background**: conventional notion stating that trust originates from informed, rational, and consequential judgments is challenged by the idea of an "affect-based" form of (political) trust (e.g., Theiss-Morse and Barton 2017^[Theiss-Morse, E., and D. Barton. 2017. “Emotion, Cognition, and Political Trust.” Pp. 160–75 in Handbook on Political Trust. Edward Elgar Publishing.])

- **Research Question**: Are individual trust judgments in surveys driven by affective rationales?

- **Questionnaire Design**: closed-ended political trust question followed by open-ended probing question; voice condition only

- **Data**: U.S. non-probability sample; n=1,276


</font size>


------------------------------------------------------------------------

## Study 3: Methodology

<font size = "6">

- Operationalization via sentiment and emotion analysis

- Transcript-based
	- pysentimiento for sentiment recognition (Pérez et al. 2023^[Pérez, J. et al. 2023. “Pysentimiento: A Python Toolkit for Opinion Mining and Social NLP Tasks.” arXiv.])
	- zero-shot prompting with GPT-3.5-turbo
- Speech-based
	- SpeechBrain for Speech Emotion Recognition (Ravanelli et al. 2021^[Ravanelli, M. et al. 2021. “SpeechBrain: A General-Purpose Speech Toolkit.” arXiv.])
	
	
</font size>


------------------------------------------------------------------------


## Study 3: Results

<div style="text-align: center;">
<figure>
  <img src="paper3/results-emotion.png" width="650" height="550">
  <figcaption>Figure 3: Emotion Recognition for Speech Data with SpeechBrain. <em>Note.</em> CIs are 95%, n_neutral=408, n_anger=44, n_sadness=18, n_happiness=21.</figcaption>
</figure>
</div>

------------------------------------------------------------------------

## Summary 

<font size = "6">

:::{.fragment}
- Web surveys allow to collect narrative answers that provide valuable insights into survey responses
	- think aloud, associations, emotions, tonal cues, additional info, etc.
:::

:::{.fragment}
- New technologies (smartphone surveys, speech-to-text algorithms) can be used to collect such data in innovative ways (e.g., spoken answers) (consider your population!)
:::

:::{.fragment}
- Analyzing natural language can inform various debates, e.g.:
	- [Study 1]{style="color:blue"}: equivalence debate in trust research
	- [Study 3]{style="color:blue"}: cognitive-versus-affective debate in political trust research
	- [Study 2]{style="color:blue"}: survey questionnaire design or item and data quality in general (e.g., associations, sentiment, emotions) ([Study 1-3]{style="color:blue"})
:::

</font size>

------------------------------------------------------------------------

## Machine Learning and Open-ended Answers

<font size = "6">

<blockquote style="color:blue">Large Language models (LLMs) facilitate the accessibility and implementation of semi-automated methods.</blockquote>

:::{.fragment}
- traditional semi-automated methods, such as supervised ML, are helpful and appealing, but they require sufficient and high-quality training data (i.e., labeled examples)

- E.g., [Study 1]{style="color:orange"}: Random Forest with 1,500 labeled examples versus BERT
<!-- higher accuracy for BERT (87-95) than RF (83-92) for the same set of labeled documents -->

- this can be a challenge for survey researchers when surveys don't provide thousands of documents
<!-- issues of item response rates to OEQs, experimental conditions, structure of open answer, etc. -->

- LLMs allow researchers to access and leverage great capabilities without having to build complex systems from scratch
:::


------------------------------------------------------------------------

## Machine Learning and Open-ended Answers

<blockquote style="color:blue">Fine-tuning pre-trained models can be valuable for classifying domain-specific data.</blockquote>

:::{.fragment}
- LLMs are already pre-trained on vast amounts of text
- fine-tuning requires little resources and can add domain-specific context
	- [Study 1]{style="color:orange"}: Fine-tuning with ~20% (n=1,500) documents shows high accuracy (95%) ("known-unknown others" classification)
:::

:::{.fragment}
- But: consider the complexity and limited transparency of these models
	- always start with simple methods and evaluate
		- [Study 1]{style="color:orange"}: Random Forest &rarr; BERT
		- [Study 3]{style="color:orange"}: dictionary approach &rarr; deep learning
	- accuracy-explainability trade-off
:::

</font size>

------------------------------------------------------------------------

## Machine Learning and Open-Ended Answers

<font size = "6">

<blockquote style="color:blue">Increasing number of possibilities to reduce manual input to a minimum.</blockquote>

:::{.fragment}
-  [Study 3]{style="color:orange"}: zero-shot prompting result in similar findings than fine-tuned versions of pre-trained models (e.g., overlap of 80% of GPT-prompting vs. pysentimiento)
:::

:::{.fragment}
- deciding on a suitable number of manual examples depends on various factors such as the task difficulty 
:::

:::{.fragment}
- the lesser the manual input, the more important the manual inspection of results (e.g., [Study 2]{style="color:orange"}: what are high-entropy documents?)
<!-- (Study 2 entropy, study 3 sentiment with simple dict approach) -->
<!-- Study 3: which voice answers sound happy -->
:::

</font size>



------------------------------------------------------------------------

## Fully manual, semi-automated, or fully automated?

<font size = "6">

The final decision for one of the approaches depends on:

:::{.fragment}
- difficulty of the given task (e.g., general versus specific codes)

- size of the available dataset (e.g., n, splits by experimental conditions)

- structure of the open answers (e.g., length, amount of context &rarr; this depends on the question design)

- the amount and state of previous research (e.g., available code schemes)

- desired accuracy and desired transparency

- available resources (e.g., human power, computational power (GPU), time resources)
:::
</font size>




------------------------------------------------------------------------

## <font size = "14"> Thank you for your Attention! </font size> {.center background-image="overview-3-studies.png" background-size="130%" background-opacity="0.4"}


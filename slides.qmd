---
title: "Methods for the Classification of Data from Open-Ended Questions in Surveys"
subtitle: "Disputation<br>16 April 2024"
author:
  - name: Camille Landesvatter
    affiliations:
      - name: University of Mannheim
format:
  revealjs: 
    theme: [white, custom.css]
    #footer: "Landesvatter: Methods for the Classification of Data from Open-Ended Questions in Surveys"
    embed-resources: true
    slide-number: true
    include-after-body: 
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn });
          });
          </script>
    preview-links: auto
    #logo: images/quarto.png
editor: 
  markdown: 
    wrap: 72
---

## Research Questions

<!-- "Welcome to my disputation of my thesis with the title .. " -->

<font size = "6">

<blockquote>[Which methods can we use to classify data from open-ended
survey questions?]{style="color:black"}</blockquote>

<blockquote>[Can we leverage these methods to make empirical
contributions to different research areas?]{style="color:black"}</blockquote>

</font size>

------------------------------------------------------------------------

## Motivation

<font size = "6">

::: fragment
1️⃣ Increase in methods to collect natural language [(e.g., smartphone
surveys with voice technologies)]{style="color:blue"} requires the evaluation of available classification methods [(i.e., fully manual, semi-automated, fully automated methods)]{style="color:blue"}.
:::

::: fragment
2️⃣ Special structure of open-ended survey answers [(e.g., shortness, lack
of context)]{style="color:blue"} requires the testing of machine learning methods for the survey context.
:::

:::fragment 
3️⃣ The potential of open answers to equip researchers with rich data useful for various research areas and debates.
:::

</font size>

------------------------------------------------------------------------

## Overview of Studies

<font size = "6">

+-------------------------+-----------------------+------------------+
| Study 1                 | Study 2               | Study 3          |
+=========================+=======================+==================+
| How valid are trust     | Open-ended survey     | Asking Why: Is   |
| survey measures? New    | questions: A          | there an         |
| insights from           | comparison of         | Affective        |
| open-ended probing data | information content   | Component of     |
| and supervised machine  | in text and audio     | Political Trust  |
| learning                | response formats      | Ratings in       |
|                         |                       | Surveys?         |
+-------------------------+-----------------------+------------------+


+-------------------------+-----------------------+--------------------+
| Research Areas/Debates  |                       |                    |
+=========================+=======================+====================+
| Measurement equivalence | Questionnaire Design  | Emotion Analysis   |
+-------------------------+-----------------------+--------------------+

</font size>

------------------------------------------------------------------------

## <font size = "10"> Study 1:<br>"How valid are trust survey measures?<br>New insights from open-ended probing data and supervised machine learning"<br>(Published in Sociological Methods & Research)</font size> {.center}


<!-- The first study is titled "..." and here I investigated whether we can rely on supervised machine learning to learn something about the measurement validity of survey questions -->

------------------------------------------------------------------------

## Study 1: Characteristics

<font size = "6">

-   **Background**: debate about whether we are measuring the same type of trust across respondents (i.e., equivalence debate cf. Bauer & Freitag 2018)

-   **Research Question**: How valid are traditional trust survey
    measures?

-   **Questionnaire Design**: 5 open-ended questions per respondent,
    block-randomized order

-   **Data**: U.S. non-probability sample; $n$=1,500 with 7,497 open
    answers

</font size>

------------------------------------------------------------------------

## Study 1: Methodology

<font size = "6">

<figure>

<img src="paper1/methods.png" width="850" height="270"/>

<figcaption>Figure 1: Supervised Classification for a Trust
Question.</figcaption>

</figure>

::: fragment
Supervised classification approach:

-   

    1.  manual labeling of randomly sampled documents
        (n=\[[1,000]{style="color:orange"}/[1,500]{style="color:blue"}\])

-   

    2.  fine-tuning the weights of two BERT[^2] models, using the
        manually coded data as training data, to classify the remaining
        n=\[[6,500]{style="color:orange"}/[6,000]{style="color:blue"}\]
:::

[^2]: bidirectional encoder representations from transformers

<!-- - accuracy^[accuracy = correct predictions / all predictions]: 87% (I) and 95% (II) -->

</font size>

------------------------------------------------------------------------

## Study 1: Results

<figure>
  <table style="font-size: 20px;">
    <tbody>
      <tr style="height: 40px;">
        <td style="height: 40px;"><strong>ID</strong></td>
        <td><strong>Measure</strong></td>
        <td><strong>Trust</strong></td>
        <td><strong>Probing Answer</strong></td>
        <td><strong>Associations (known others)</strong></td>
        <td><strong>Associations (sentiment)</strong></td>
      </tr>
      <tr style="height: 20px; background-color: #cfe2d4;">
        <td style="height: 20px;">123</td>
        <td>Most people</td>
        <td>0.33</td>
        <td>I was thinking of people I don't know personally.</td>
        <td>0 (No)</td>
        <td>0 (neutral/positive)</td>
      </tr>
      <tr style="height: 20px; background-color: #cfe2d4;">
        <td style="height: 20px;">3139</td>
        <td>Most people</td>
        <td>0.17</td>
        <td>Tourists that come to our little village. I tend to be very wary of them.</td>
        <td>0 (No)</td>
        <td>1 (negative)</td>
      </tr>
      <tr style="height: 20px; background-color: #fff2cc;">
        <td style="height: 20px;">2980</td>
        <td>Stranger</td>
        <td>0</td>
        <td>No one in particular, but I don't think I could trust anyone ever again.</td>
        <td>0 (No)</td>
        <td>1 (negative)</td>
      </tr>
      <tr style="height: 40px; background-color: #d9e1f2;">
        <td style="height: 40px;">4286</td>
        <td>Watching a loved one</td>
        <td>0</td>
        <td>A former neighbor of mine who was a single father with a son close to my son's age.</td>
        <td>1 (Yes)</td>
        <td>0 (neutral/positive)</td>
      </tr>
    </tbody>
  </table>
 <figcaption>Table 2: Illustration of exemplary data. <em>Note:</em> n=7,497.</figcaption>
</figure>


::: fragment
<figure>

<img src="paper1/regression-plot-disputation-short.png"/>

<figcaption>Figure 2: Associations and Trust Scores. <em>Note.</em> CIs are 95% and 90%.</figcaption>

</figure>
:::


------------------------------------------------------------------------

## <font size = "10"> Study 2:<br>"Open-ended survey questions: A comparison of information content in text and audio response formats"<br>(Under Review at Public Opinion Quarterly)</font size> {background-image="paper2/typing-vs-speaking.png" background-size="60%" background-opacity="0.1" .center}

<!-- The second study is titled "..." and here I investigated whether we can use computational measures and ML to inform survey questionnaire design research -->

------------------------------------------------------------------------

## Study 2: Characteristics

<font size = "6">

-   **Background**: requests for spoken answers are assumed to trigger
    an open narration with more intuitive and spontaneous answers (e.g.,
    Gavras et al. 2022)

-   **Research Question**: Are there differences in information content
    between responses given in audio and text formats?

-   **Experimental Design**: random assignment into either the text or
    audio condition

</font size>

------------------------------------------------------------------------

## Study 2: Methodology

<font size = "6">

-   **Operationalization** of information content in open answers via
    
    - response length (# of words)
    - number of topics
    - response entropy

-   **Questionnaire Design**: 9 open-ended questions per respondent,
    block-randomized order

-   **Data**: U.S. non-probability sample; $n$=1,461 with $n_{text}$=800
    and $n_{audio}$=661

    -   average item non-response rate text: 1%
    -   average item non-response rate audio: 53%

</font size>

<!-- entropy: how much new information does the document contain, we are the first -->

------------------------------------------------------------------------

## Study 2: Results

::: {style="text-align: center;"}
<figure>

<img src="paper2/information-content-summary-short.png"/>

<figcaption>Figure 3: Information Content Measures across Questions.<br>
<em>Note.</em> CIs are 95%, n_vote-choice: 830 (audio: 225, text: 605),
n_future-children: 1,337 (audio: 389, text: 748)</figcaption>

</figure>
:::

<!-- data %>% -->

<!--     group_by(variable_probing, condition) %>% -->

<!--     summarise(non_missing_count = sum(!is.na(text_answer), na.rm = TRUE)) -->

------------------------------------------------------------------------

## <font size = "10"> Study 3:<br>"Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys?"<br>(Submitted to American Political Science Review)</font size> {background-image="paper3/dreamstudio-emotion-analysis.png" background-size="40%" background-opacity="0.2" .center}

<!-- The third study is titled "..." and here I investigated whether I can use sentiment and emotion recognition and importantly speech emotion recognition to learn about political trust -->

------------------------------------------------------------------------

## Study 3: Characteristics

<font size = "6">

-   **Background**: conventional notion of rational trust is challenged by the idea of an "affect-based" form of political trust (e.g., Theiss-Morse and Barton 2017)

-   **Research Question**: Are individual trust judgments in surveys
    driven by affective components?

-   **Questionnaire Design**: audio condition only

-   **Data**: U.S. non-probability sample; $n$=1,474 with 491 audio open
    answers

</font size>

<!-- I am the first to research this -->

------------------------------------------------------------------------

## Study 3: Methodology

<div style="text-align: center;">

<figure>

<img src="paper3/workflow.excalidraw.png"/>

<figcaption>Figure 4: Methods for Sentiment and Emotion
Analysis.</figcaption>


</figure>

------------------------------------------------------------------------

## Study 3: Results

::: {style="text-align: center;"}
<figure>

<img src="paper3/results-emotion-long.png" width="650" height="550"/>

<figcaption>Figure 5: Emotions in Speech Data from SpeechBrain.<br><em>Note.</em> n_neutral=408, n_anger=44, n_sadness=18,
n_happiness=21. Reference category (right): neutral.</figcaption>

</figure>
:::

<!-- mention the formal nature of survey settings as a possible explanation and that speechbrain hasnt been used in social science research so far and we need more applications with other survey datasets -->

------------------------------------------------------------------------

## Summary and Conclusions

<font size = "6">

<!-- mention that first summary, than remaining minutes on methodological (!!) key takeaways  -->

::: fragment
-   Web surveys allow to collect narrative answers that provide valuable
    insights into survey responses
    -   think aloud, associations, emotions, tonal cues, additional
        info, etc.
:::

::: fragment
-   New technologies (e.g., speech recognition) allow innovative data collection
:::

::: fragment
-   Analyzing natural language can inform various debates, e.g.:
    -   [Study 1]{style="color:blue"}: equivalence debate in trust
        research (cf. Bauer & Freitag 2018)
    -   [Study 2]{style="color:blue"}: audio response formats in web surveys (cf. Gavras et al. 2022)
    -   [Study 3]{style="color:blue"}: cognitive-versus-affective debate
        in political trust research (cf. Theiss-Morse and Barton 2017)
    -   [Study 1-3]{style="color:blue"}: item and data quality
:::

</font size>

------------------------------------------------------------------------

## Summary and Conclusions
### Semi-automated methods for open survey answers

<!-- For my Conclusion, I want to emphasize the usefulness of semi-automated methods for social science application -->

<font size = "6">

::: fragment
- traditional supervised methods need extensive labeled datasets
:::

::: fragment
- Language Models (LMs) allow modeling with less labeled data and domain-specific knowledge (fine-tuning and prompting techniques)
	-   E.g., [Study 1]{style="color:orange"}: Random Forest vs. BERT with n=1,500
:::

::: fragment
- But: LMs suffer from high complexity and limited transparency 
    -   start with simple methods and evaluate (e.g., dictionary approach → deep
            learning in [Study 3]{style="color:orange"})
    -   trade off between accuracy and explainability
:::

::: fragment
- additionally consider: task difficulty, sample size, structure of the answers, state of previous research, available resources, etc.
:::


</font size>

<!-- traditional methods often dont work as supposed when we dont have enough training data, because surveys often don't provide thousands of documents -->
<!-- issues of item response rates to OEQs, experimental conditions, structure of open answer, etc. -->
<!-- higher accuracy for BERT (87-95) than RF (83-92) for the same set of labeled documents -->

<!-- - difficulty of the given task (e.g., general versus specific codes) -->

<!-- -   size of the available dataset (e.g., n, splits by experimental -->
<!--     conditions) -->

<!-- -   structure of the open answers (e.g., length, amount of context → -->
<!--     this depends on the question design) -->

<!-- -   the amount and state of previous research (e.g., available code -->
<!--     schemes) -->

<!-- -   desired accuracy and desired transparency -->

<!-- -   available resources (e.g., human power, computational power (GPU), -->
<!--     time resources) -->

------------------------------------------------------------------------

## <font size = "14"> Thank you for your Attention! </font size> {.center}

------------------------------------------------------------------------

## References {data-hide-slide-number="true"}

<font size = "5">

Bauer, P. C., and M. Freitag. 2018. “Measuring Trust.” Pp. 1–27 in
    The Oxford Handbook of Social and Political Trust, edited by E. M.
    Uslaner. Oxford University Press.

Gavras, K. et al. 2022. “Innovating the collection of open-ended
    answers: The linguistic and content characteristics of written and
    oral answers to political attitude questions.” Journal of the Royal
    Statistical Society. Series A, 185(3):872-890.
    
Pérez, J. et al. 2023. “Pysentimiento: A Python Toolkit for
    Opinion Mining and Social NLP Tasks.” arXiv.<br>Ravanelli, M. et al.
    2021. “SpeechBrain: A General-Purpose Speech Toolkit.” arXiv
    
Theiss-Morse, E., and D. Barton. 2017. “Emotion, Cognition, and
    Political Trust.” Pp. 160–75 in Handbook on Political Trust. Edward
    Elgar Publishing.
    
</font size>
---
title: "Methods for the Classification of Data from Open-Ended Questions in Surveys"
subtitle: "Disputation<br>16 April 2024"
author:
  - name: Camille Landesvatter
    affiliations:
      - name: University of Mannheim
format:
  revealjs: 
    theme: [white, custom.css]
    #footer: "Landesvatter: Methods for the Classification of Data from Open-Ended Questions in Surveys"
    embed-resources: true
    slide-number: true
    include-after-body: 
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn });
          });
          </script>
    preview-links: auto
    #logo: images/quarto.png
editor: 
  markdown: 
    wrap: 72
---

## Research Questions

<!-- "Welcome to my disputation of my thesis with the title .. " -->

<font size = "6">

<blockquote>[Which methods can we use to classify data from open-ended
survey questions?]{style="color:black"}</blockquote>

<blockquote>[Can we leverage these methods to make empirical
contributions to substantial
questions?]{style="color:black"}</blockquote>

</font size>

------------------------------------------------------------------------

## Motivation

<font size = "6">

::: fragment
1️⃣ Increase in methods to collect natural language (e.g., smartphone
surveys with voice technologies) requires the evaluation of available classification methods.
:::

::: fragment
2️⃣ Special structure of open-ended survey answers (e.g., shortness, lack
of context) requires the testing of machine learning methods for the survey context.
:::

::: fragment
- Fully manual: no automation
- Semi-automated: supervised ML, pre-trained models, prompt-based learning
- Fully automated: unsupervised ML, clustering methods, topic models
:::

:::fragment 
3️⃣ Open answers have the potential to equip researchers with rich data useful for various subjects of research.
:::

</font size>

------------------------------------------------------------------------

## Overview of Studies

<font size = "6">

+-------------------------+-----------------------+------------------+
| Study 1                 | Study 2               | Study 3          |
+=========================+=======================+==================+
| How valid are trust     | Open-ended survey     | Asking Why: Is   |
| survey measures? New    | questions: A          | there an         |
| insights from           | comparison of         | Affective        |
| open-ended probing data | information content   | Component of     |
| and supervised machine  | in text and audio     | Political Trust  |
| learning                | response formats      | Ratings in       |
|                         |                       | Surveys?         |
+-------------------------+-----------------------+------------------+


+-------------------------+-----------------------+--------------------+
| Research Fields         |                       |                    |
+=========================+=======================+====================+
| Measurement equivalence | Questionnaire Design  | Emotion Analysis   |
+-------------------------+-----------------------+--------------------+


</font size>

------------------------------------------------------------------------

## <font size = "10"> Study 1:<br>"How valid are trust survey measures? New insights from open-ended probing data and supervised machine learning"<br>(Published in Sociological Methods & Research)</font size> {background-image="paper1/theory.png" background-size="80%" background-opacity="0.2"}

<!-- The first study is titled "..." and here I investigated whether we can rely on supervised machine learning to learn something about the measurement validity of survey questions -->

------------------------------------------------------------------------

## Study 1: Characteristics

<font size = "6">

-   **Background**: ongoing debates about which type of trust survey
    researchers are measuring with traditional survey items (i.e.,
    equivalence debate cf. Bauer & Freitag 2018)

-   **Research Question**: How valid are traditional trust survey
    measures?

-   **Questionnaire Design**: 5 open-ended questions per respondent,
    block-randomized order

-   **Data**: U.S. non-probability sample; $n$=1,500 with 7,497 open
    answers

</font size>

------------------------------------------------------------------------

## Study 1: Methodology

<font size = "6">

<figure>

<img src="paper1/methods.png" width="850" height="270"/>

<figcaption>Figure 1: Supervised Classification for a Trust
Question.</figcaption>

</figure>

::: fragment
Supervised classification approach:

-   

    1.  manual labeling of randomly sampled documents
        (n=\[[1,000]{style="color:orange"}/[1,500]{style="color:blue"}\])

-   

    2.  fine-tuning the weights of two BERT[^2] models, using the
        manually coded data as training data, to classify the remaining
        n=\[[6,500]{style="color:orange"}/[6,000]{style="color:blue"}\]
:::

[^2]: bidirectional encoder representations from transformers

<!-- - accuracy^[accuracy = correct predictions / all predictions]: 87% (I) and 95% (II) -->

</font size>

------------------------------------------------------------------------

## Study 1: Results

<figure>
  <table style="font-size: 20px;">
    <tbody>
      <tr style="height: 40px;">
        <td style="height: 40px;"><strong>ID</strong></td>
        <td><strong>Measure</strong></td>
        <td><strong>Trust</strong></td>
        <td><strong>Probing Answer</strong></td>
        <td><strong>Associations (known others)</strong></td>
        <td><strong>Associations (sentiment)</strong></td>
      </tr>
      <tr style="height: 20px; background-color: #cfe2d4;">
        <td style="height: 20px;">123</td>
        <td>Most people</td>
        <td>0.33</td>
        <td>I was thinking of people I don't know personally.</td>
        <td>0 (No)</td>
        <td>0 (neutral/positive)</td>
      </tr>
      <tr style="height: 20px; background-color: #cfe2d4;">
        <td style="height: 20px;">3139</td>
        <td>Most people</td>
        <td>0.17</td>
        <td>Tourists that come to our little village. I tend to be very wary of them.</td>
        <td>0 (No)</td>
        <td>1 (negative)</td>
      </tr>
      <tr style="height: 20px; background-color: #fff2cc;">
        <td style="height: 20px;">2980</td>
        <td>Stranger</td>
        <td>0</td>
        <td>No one in particular, but I don't think I could trust anyone ever again.</td>
        <td>0 (No)</td>
        <td>1 (negative)</td>
      </tr>
      <tr style="height: 40px; background-color: #d9e1f2;">
        <td style="height: 40px;">4286</td>
        <td>Watching a loved one</td>
        <td>0</td>
        <td>A former neighbor of mine who was a single father with a son close to my son's age.</td>
        <td>1 (Yes)</td>
        <td>0 (neutral/positive)</td>
      </tr>
    </tbody>
  </table>
 <figcaption>Table 2: Illustration of exemplary data. <em>Note:</em> n=7,497.</figcaption>
</figure>


::: fragment
<figure>

<img src="paper1/regression-plot-disputation-short.png"/>

<figcaption>Figure 2: Associations and Trust Scores. <em>Note.</em> CIs are 95% and 90%.</figcaption>

</figure>
:::


------------------------------------------------------------------------

## <font size = "10"> Study 2:<br>"Open-ended survey questions: A comparison of information content in text and audio response formats"<br>(Submitted to Public Opinion Quarterly)</font size> {background-image="paper2/typing-vs-speaking.png" background-size="60%" background-opacity="0.2"}

<!-- The second study is titled "..." and here I investigated whether we can use computational measures and ML to inform survey questionnaire design research -->

------------------------------------------------------------------------

## Study 2: Characteristics

<font size = "6">

-   **Background**: requests for spoken answers are assumed to trigger
    an open narration with more intuitive and spontaneous answers (e.g.,
    Gavras et al. 2022)

-   **Research Question**: Are there differences in information content
    between responses given in voice and text formats?

-   **Experimental Design**: random assignment into either the text or
    voice condition

</font size>

------------------------------------------------------------------------

## Study 2: Methodology

<font size = "6">

-   **Operationalization** of information content in open answers via
    application of measures from information theory and machine learning

    -   response length, number of topics, response entropy

-   **Questionnaire Design**: 9 open-ended questions per respondent,
    block-randomized order

-   **Data**: U.S. non-probability sample; $n$=1,461 with $n_{text}$=800
    and $n_{audio}$=661

    -   average item non-response rate text: 1%
    -   average item non-response rate audio: 53%

</font size>

------------------------------------------------------------------------

## Study 2: Results

::: {style="text-align: center;"}
<figure>

<img src="paper2/information-content-summary-short.png"/>

<figcaption>Figure 3: Information Content Measures across Questions.<br>
<em>Note.</em> CIs are 95%, n_vote-choice: 830 (audio: 225, text: 605),
n_future-children: 1,337 (audio: 389, text: 748)</figcaption>

</figure>
:::

<!-- data %>% -->

<!--     group_by(variable_probing, condition) %>% -->

<!--     summarise(non_missing_count = sum(!is.na(text_answer), na.rm = TRUE)) -->

<!-- While the finding that voice answers have a higher response length is common knowledge, we could contribute to the finding that other measures of information content, such as the number of topics, support voice answer -->

------------------------------------------------------------------------

## <font size = "10"> Study 3:<br>"Asking Why: Is there an Affective Component of Political Trust Ratings in Surveys?"<br>(Submitted to American Political Science Review)</font size> {background-image="paper3/dreamstudio-emotion-analysis.png" background-size="40%" background-opacity="0.2"}

<!-- The third study is titled "..." and here I investigated whether I can use sentiment and emotion recognition and importantly speech emotion recognition to learn about political trust -->

------------------------------------------------------------------------

## Study 3: Characteristics

<font size = "6">

-   **Background**: conventional notion stating that trust originates
    from informed, rational, and consequential judgments is challenged
    by the idea of an "affect-based" form of (political) trust (e.g.,
    Theiss-Morse and Barton 2017)

-   **Research Question**: Are individual trust judgments in surveys
    driven by affective rationales?

-   **Questionnaire Design**: voice condition only

<!-- - **Questionnaire Design**: closed-ended political trust question followed by open-ended probing question; voice condition only -->

-   **Data**: U.S. non-probability sample; $n$=1,474 with 491 audio open
    answers

</font size>

------------------------------------------------------------------------

## Study 3: Methodology

<div style="text-align: center;">

<figure>

<img src="paper3/workflow.excalidraw.png"/>

<figcaption>Figure 4: Methods for Sentiment and Emotion
Analysis.</figcaption>


</figure>

------------------------------------------------------------------------

## Study 3: Results

::: {style="text-align: center;"}
<figure>

<img src="paper3/results-emotion-long.png" width="650" height="550"/>

<figcaption>Figure 5: Emotions in Speech Data from SpeechBrain.<br><em>Note.</em> n_neutral=408, n_anger=44, n_sadness=18,
n_happiness=21. Reference category (right): neutral.</figcaption>

</figure>
:::

<!-- mention the formal nature of survey settings as a possible explanation and that speechbrain hasnt been used in social science research so far and we need more applications with other survey datasets -->

------------------------------------------------------------------------

## Summary and Conclusions

<font size = "6">

<!-- mention that first summary, than remaining minutes on methodological (!!) key takeaways  -->

::: fragment
-   Web surveys allow to collect narrative answers that provide valuable
    insights into survey responses
    -   think aloud, associations, emotions, tonal cues, additional
        info, etc.
:::

::: fragment
-   New technologies (e.g., speech recognition) allow innovative data collection
:::

::: fragment
-   Analyzing natural language can inform various debates, e.g.:
    -   [Study 1]{style="color:blue"}: equivalence debate in trust
        research (cf. Bauer & Freitag 2018)
    -   [Study 2]{style="color:blue"}: oral response formats in web surveys (cf. Gavras et al. 2022)
    -   [Study 3]{style="color:blue"}: cognitive-versus-affective debate
        in political trust research (cf. Theiss-Morse and Barton 2017)
    -   [Study 1-3]{style="color:blue"}: item and data quality in
        general (e.g., associations, information content, sentiment,
        emotions)
:::

</font size>

------------------------------------------------------------------------

## Summary and Conclusions
### Semi-automated methods for open survey answers

<!-- For my Conclusion, I want to emphasize the usefulness of semi-automated methods for social science application -->

<font size = "6">

::: fragment
- supervised machine learning requires sufficient and high-quality training data (i.e., labeled examples)
:::

::: fragment
- LLMs allow modeling with fewer training data and domain-specific knowledge (fine-tuning and prompting techniques)

	-   E.g., [Study 1]{style="color:orange"}: BERT outperforms Random Forest with 1,500 labeled examples
:::

</font size>

<!-- traditional methods often dont work as supposed when we dont have enough training data, because surveys often don't provide thousands of documents -->
<!-- issues of item response rates to OEQs, experimental conditions, structure of open answer, etc. -->
<!-- higher accuracy for BERT (87-95) than RF (83-92) for the same set of labeled documents -->

------------------------------------------------------------------------

## Summary and Conclusions

<font size = "6">

- But LLMs suffer from high complexity and limited transparency 
    -   start with simple methods and evaluate
        -   [Study 1]{style="color:orange"}: Random Forest → BERT
        -   [Study 3]{style="color:orange"}: dictionary approach → deep
            learning
    -   trade off between accuracy and explainability

::: fragment
- Fully manual, semi-automated, or fully automated?
	- task difficulty, sample size, structure of the answers, state of previous research, accuracy and transparency, available resources 
:::

</font size>

<!-- - difficulty of the given task (e.g., general versus specific codes) -->

<!-- -   size of the available dataset (e.g., n, splits by experimental -->
<!--     conditions) -->

<!-- -   structure of the open answers (e.g., length, amount of context → -->
<!--     this depends on the question design) -->

<!-- -   the amount and state of previous research (e.g., available code -->
<!--     schemes) -->

<!-- -   desired accuracy and desired transparency -->

<!-- -   available resources (e.g., human power, computational power (GPU), -->
<!--     time resources) -->

------------------------------------------------------------------------

## <font size = "14"> Thank you for your Attention! </font size> {.center background-image="overview-3-studies.png" background-size="150%" background-opacity="0.4"}

------------------------------------------------------------------------

## References {data-hide-slide-number="true"}

<font size = "5">

Bauer, P. C., and M. Freitag. 2018. “Measuring Trust.” Pp. 1–27 in
    The Oxford Handbook of Social and Political Trust, edited by E. M.
    Uslaner. Oxford University Press.

Gavras, K. et al. 2022. “Innovating the collection of open-ended
    answers: The linguistic and content characteristics of written and
    oral answers to political attitude questions.” Journal of the Royal
    Statistical Society. Series A, 185(3):872-890.
    
Pérez, J. et al. 2023. “Pysentimiento: A Python Toolkit for
    Opinion Mining and Social NLP Tasks.” arXiv.<br>Ravanelli, M. et al.
    2021. “SpeechBrain: A General-Purpose Speech Toolkit.” arXiv
    
Theiss-Morse, E., and D. Barton. 2017. “Emotion, Cognition, and
    Political Trust.” Pp. 160–75 in Handbook on Political Trust. Edward
    Elgar Publishing.
    
</font size>